{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from pathlib import Path\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import json\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    ")\n",
    "\n",
    "# I found that a batch size of 128 offers the best trade-off between\n",
    "# model training time and batch volatility.\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19368 images belonging to 10 classes.\n",
      "Found 4836 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'D:/Downloads/Copy of shopee-product-detection-dataset/train_1st/train_1st/',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    ")\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'D:/Downloads/Copy of shopee-product-detection-dataset/train_1st/train_1st/',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "# include top should be False to remove the softmax layer\n",
    "\n",
    "\n",
    "pretrained_model = VGG16(include_top=False, weights='imagenet',input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_features_train = pretrained_model.predict(train_generator)\n",
    "vgg_features_val = pretrained_model.predict(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = to_categorical(train_generator.labels)\n",
    "val_target = to_categorical(validation_generator.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(pretrained_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', name='Dense_Intermediate'))\n",
    "model.add(Dropout(0.1, name='Dropout_Regularization'))\n",
    "model.add(Dense(10, activation='softmax', name='Output'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', metrics=['accuracy'], loss='categorical_crossentropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "labels_count = dict()\n",
    "for img_class in [ic for ic in os.listdir('D:/Downloads/Copy of shopee-product-detection-dataset/train_1st/train_1st/') if ic[0] != '.']:\n",
    "    labels_count[img_class] = len(os.listdir('D:/Downloads/Copy of shopee-product-detection-dataset/train_1st/train_1st/' + img_class))\n",
    "total_count = sum(labels_count.values())\n",
    "class_weights = {cls: total_count / count for cls, count in \n",
    "                 enumerate(labels_count.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "151/151 [==============================] - 10855s 72s/step - loss: 24.0126 - accuracy: 0.1054 - val_loss: 2.3033 - val_accuracy: 0.1107\n",
      "Epoch 2/20\n",
      "151/151 [==============================] - 10751s 71s/step - loss: 23.0283 - accuracy: 0.0972 - val_loss: 2.3028 - val_accuracy: 0.1105\n",
      "Epoch 3/20\n",
      "104/151 [===================>..........] - ETA: 43:56 - loss: 23.0164 - accuracy: 0.0991"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator.filenames) // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(train_generator.filenames) // batch_size,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[\n",
    "        EarlyStopping(patience=3, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(patience=2)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
